{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_OZ6vlC-hBIM"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B0iG2J9jhRS5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UERMljvIhSir"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5IxU2wUdsYL",
        "outputId": "54ce43c7-825f-494b-828e-44f4f30042a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pO8nvi9hkqs"
      },
      "source": [
        "\n",
        "# MinHashLSH Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AwPynAefbOLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.sql import SparkSession, DataFrame, Row\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import ArrayType, StringType, IntegerType, FloatType\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wTv3COYHxpCA"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "                    .appName(\"Progress 2\") \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "documents = spark.read.text('WebOfScience-5736.txt', lineSep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RXhBbr-cf3Xz"
      },
      "outputs": [],
      "source": [
        "class MinHashLSH:\n",
        "    def __init__(self,\n",
        "                 documents: DataFrame,\n",
        "                 numHashTables: int = 100,\n",
        "                 numBands: int = 50,\n",
        "                 k: int = 4):\n",
        "        self.documents = self.processDocuments(documents)\n",
        "\n",
        "        self.vocab = None\n",
        "        self.vocab_size = None\n",
        "        self.bool_vectors = None\n",
        "        self.buckets = None\n",
        "\n",
        "        # config\n",
        "        self.k = k\n",
        "        self.numBands = numBands\n",
        "        self.HASH_PRIME = 2038074743\n",
        "        self.numHashTables = numHashTables\n",
        "\n",
        "    @staticmethod\n",
        "    def processDocuments(documents):\n",
        "        documents_id = documents.rdd.zipWithIndex() \\\n",
        "                      .map(lambda row_index: (row_index[1],) + tuple(row_index[0])) \\\n",
        "                      .toDF(['id', 'value'])\n",
        "\n",
        "        documents_id = documents_id.withColumn('value', F.lower(F.col('value'))) \\\n",
        "                                    .withColumn(\"value\", F.regexp_replace(F.col(\"value\"), '[^a-zA-Z0-9\\s]', ''))\n",
        "\n",
        "\n",
        "        return documents_id\n",
        "\n",
        "    def build_vocab(self, shingle: DataFrame) -> DataFrame:\n",
        "        # Remove duplicate shingle\n",
        "        windowSpec = Window.orderBy('shingle')\n",
        "        vocab = shingle.select('shingle').distinct()\n",
        "        vocab = vocab.withColumn('shingle_id', F.row_number().over(windowSpec) - 1)\n",
        "        self.vocab_size = vocab.count()\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def multiHotEncoding(self, shingle):\n",
        "        vocab = self.vocab\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        sparse_vector = shingle.join(vocab, on='shingle', how='left') \\\n",
        "                                .groupby('id') \\\n",
        "                                .agg(F.collect_set('shingle_id').alias('shingle_ids'))\n",
        "\n",
        "        # Define a UDF to convert a list to a one-hot encoded SparseVector\n",
        "        @F.udf(returnType=VectorUDT())\n",
        "        def one_hot_encode(ids):\n",
        "            return Vectors.sparse(vocab_size, sorted(ids), [1]*len(ids))\n",
        "\n",
        "\n",
        "        boolean_vectors = sparse_vector.withColumn(\"features\", one_hot_encode(F.col('shingle_ids')))\n",
        "        return boolean_vectors.select('id', 'features')\n",
        "\n",
        "\n",
        "    def shingling(self, documents: DataFrame) -> DataFrame:\n",
        "        k = self.k\n",
        "\n",
        "        # Define a UDF to generate shingles\n",
        "        shingle_udf = F.udf(lambda text: [text[i:i+k] for i in range(len(text) - k + 1)], ArrayType(StringType()))\n",
        "\n",
        "        # Generate shingles and build vocabulary\n",
        "        shingles = documents.withColumn('shingles', shingle_udf(F.col('value')))\n",
        "        shingle = shingles.select('id', F.explode('shingles').alias('shingle'))\n",
        "        self.vocab = F.broadcast(self.build_vocab(shingle))\n",
        "\n",
        "        # Convert to boolean vectors\n",
        "        boolean_vectors = self.multiHotEncoding(shingle)\n",
        "        return boolean_vectors\n",
        "\n",
        "    def minhashing(self, bool_vectors) -> DataFrame:\n",
        "        random.seed(1205)\n",
        "        prime = self.HASH_PRIME\n",
        "\n",
        "        randCoefs = [\n",
        "                      (1 + random.randint(0, self.HASH_PRIME - 1), random.randint(0, self.HASH_PRIME - 1))\n",
        "                      for _ in range(self.numHashTables)\n",
        "                    ]\n",
        "\n",
        "        @F.udf(returnType=ArrayType(VectorUDT()))\n",
        "        def minHash(features):\n",
        "            indices = features.indices.tolist()\n",
        "            min_hashes = []\n",
        "            for a, b in randCoefs:\n",
        "                hash_vals = [(a*(i + 1) + b) % prime for i in indices]\n",
        "                min_hash = min(hash_vals)\n",
        "                min_hashes.append(Vectors.dense([min_hash]))\n",
        "            return min_hashes\n",
        "\n",
        "        signature = bool_vectors.withColumn('hashes', minHash(F.col('features')))\n",
        "        return signature\n",
        "\n",
        "    def locality_sensity_hashing(self, signatures: list) -> DataFrame:\n",
        "        HASH_PRIME = self.HASH_PRIME\n",
        "        numBands = self.numBands\n",
        "\n",
        "        assert numBands < self.numHashTables\n",
        "        # Number of signature values in each band\n",
        "        signa_length = self.numHashTables\n",
        "        r = int(self.numHashTables / numBands)\n",
        "\n",
        "        # Define a UDF to split the signatures into bands\n",
        "        @F.udf(ArrayType(IntegerType()))\n",
        "        def hash_bands(signature):\n",
        "            hashed_bands = []\n",
        "            for i in range(0, signa_length, r):\n",
        "                band = signature[i:i + r]\n",
        "                hashed_bands.append(hash(tuple(band)) % HASH_PRIME)\n",
        "            return hashed_bands\n",
        "\n",
        "        # Hash the bands into buckets\n",
        "        band_buckets = signatures.withColumn('bucketID', hash_bands(F.col('hashes')))\n",
        "        band_buckets = band_buckets.withColumnRenamed('hashes', 'signature')\n",
        "\n",
        "        return band_buckets\n",
        "\n",
        "    def run(self):\n",
        "        self.bool_vectors = self.shingling(self.documents).persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "        signature = self.minhashing(self.bool_vectors)\n",
        "\n",
        "        self.buckets = self.locality_sensity_hashing(signature).persist(StorageLevel.MEMORY_AND_DISK)\n",
        "        self.buckets.count()\n",
        "\n",
        "    def approxNearestNeighbors(self, key, numLim):\n",
        "        key = spark.createDataFrame([(key,)], [\"value\"])\n",
        "        key = self.processDocuments(key)\n",
        "\n",
        "        # Shingling step\n",
        "        k = self.k\n",
        "        shingle_udf = F.udf(lambda text: [text[i:i+k] for i in range(len(text) - k + 1)], ArrayType(StringType()))\n",
        "        shingles = key.withColumn('shingles', shingle_udf(F.col('value')))\n",
        "        shingle = shingles.select('id', F.explode('shingles').alias('shingle'))\n",
        "\n",
        "        key_shingles = self.multiHotEncoding(shingle)\n",
        "\n",
        "        # Min Hash step\n",
        "        key_signature = self.minhashing(key_shingles)\n",
        "\n",
        "        # LSH\n",
        "        key_bucket = self.locality_sensity_hashing(key_signature)\n",
        "        key_bucket = key_bucket.withColumnRenamed('bucketID', 'key_bucketID') \\\n",
        "                              .withColumnRenamed('id', 'key_id') \\\n",
        "                              .withColumnRenamed('features', 'key_features')\n",
        "\n",
        "\n",
        "        # Find pairs\n",
        "        candidatePairs = self.buckets.crossJoin(F.broadcast(key_bucket))\n",
        "        candidatePairs = candidatePairs.filter(F.arrays_overlap(F.col(\"bucketID\"), F.col(\"key_bucketID\")))\n",
        "\n",
        "        @F.udf(FloatType())\n",
        "        def Jaccard_Similarity(candidate, key):\n",
        "            # Convert SparseVectors to sets\n",
        "            set1 = set(candidate.indices)\n",
        "            set2 = set(key.indices)\n",
        "\n",
        "            # Calculate intersection and union\n",
        "            intersection  = len(set1.intersection(set2))\n",
        "            union = len(set1.union(set2))\n",
        "\n",
        "            # Calculate Jaccard similarity\n",
        "            return intersection / union if union != 0 else 0.0\n",
        "\n",
        "        nearestNeighbour = candidatePairs.withColumn('distCol', Jaccard_Similarity(F.col('features'), F.col('key_features')))\n",
        "\n",
        "        # Return the top n nearest neighbors\n",
        "        nearestNeighbour = nearestNeighbour.select('id', 'features', 'distCol') \\\n",
        "                                            .sort(F.col('distCol').desc()) \\\n",
        "                                            .limit(numLim)\n",
        "\n",
        "        return nearestNeighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NuqbYWf55TJ",
        "outputId": "d82cb334-8906-42c8-e4ed-c0c19b527c9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "405.8739447593689"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mh = MinHashLSH(documents)\n",
        "\n",
        "start_time = time.time()\n",
        "signature = mh.run()\n",
        "end_time = time.time()\n",
        "\n",
        "running_time = end_time - start_time\n",
        "running_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-518e-lwuFYE"
      },
      "outputs": [],
      "source": [
        "document1 = \"\"\"\n",
        "Australian pathosystems, providing the first comprehensive compilation of information\n",
        "for this continent, covering the phytoplasmas, host plants, vectors and diseases.\n",
        "Of the 33 16Sr groups reported internationally\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdSFQV0_5ebj",
        "outputId": "7f0bf866-6290-4887-905b-6fc02d4c4bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------------------+-----------+\n",
            "|  id|            features|    distCol|\n",
            "+----+--------------------+-----------+\n",
            "|   0|(103134,[706,1228...| 0.17419963|\n",
            "|5473|(103134,[2604,260...|0.093147755|\n",
            "|3527|(103134,[2725,273...| 0.08979592|\n",
            "+----+--------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_1 = mh.approxNearestNeighbors(document1, numLim = 3)\n",
        "result_1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SYVuMZ7kAQHp"
      },
      "outputs": [],
      "source": [
        "document2 = \"\"\"\n",
        "Phytoplasmas are insect-vectored bacteria that cause disease in a wide range of plant species.\n",
        "The increasing availability of molecular DNA analyses, expertise and additional methods in recent years\n",
        "has led to a proliferation of discoveries of phytoplasma-plant host associations and in\n",
        "the numbers of taxonomic groupings for phytoplasmas. The widespread use of common names based on\n",
        "the diseases with which they are associated, as well as separate phenetic and taxonomic systems\n",
        "for classifying phytoplasmas based on variation at the 16S rRNA-encoding gene, complicates interpretation of the literature.\n",
        "We explore this issue and related trends through a focus on Australian pathosystems, providing the first comprehensive compilation of information\n",
        "for this continent, covering the phytoplasmas, host plants, vectors and diseases.\n",
        "Of the 33 16Sr groups reported internationally, only groups I, II, III, X, XI and XII have been recorded in Australia\n",
        "and this highlights the need for ongoing biosecurity measures to prevent the introduction of additional pathogen groups.\n",
        "Many of the phytoplasmas reported in Australia have not been sufficiently well studied\n",
        "to assign them to 16Sr groups so it is likely that unrecognized groups and sub-groups are present.\n",
        "Wide host plant ranges are apparent among well studied phytoplasmas,\n",
        "with multiple crop and non-crop species infected by some. Disease management is further complicated by the fact\n",
        "that putative vectors have been identified for few phytoplasmas, especially in Australia.\n",
        "Despite rapid progress in recent years using molecular approaches, phytoplasmas remain\n",
        "the least well studied group of plant pathogens, making them a \"crouching tiger\" disease threat.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hog0pGm6AM6M",
        "outputId": "5d1bc193-6fcc-4f81-9f20-658b0bf3cb76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------------------+----------+\n",
            "|  id|            features|   distCol|\n",
            "+----+--------------------+----------+\n",
            "|   0|(103134,[706,1228...| 0.9830508|\n",
            "|2532|(103134,[729,2604...|0.20167653|\n",
            "|1494|(103134,[20,605,9...|0.19393939|\n",
            "|5081|(103134,[2603,261...|0.19218911|\n",
            "|3778|(103134,[256,2459...|0.18880779|\n",
            "|2234|(103134,[729,2607...|0.18124643|\n",
            "| 994|(103134,[2604,261...|0.18037602|\n",
            "|3193|(103134,[2604,260...| 0.1797235|\n",
            "|3553|(103134,[2603,260...|0.17739318|\n",
            "|5043|(103134,[882,1714...|0.17715618|\n",
            "+----+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_2 = mh.approxNearestNeighbors(document2, numLim = 10)\n",
        "result_2.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
